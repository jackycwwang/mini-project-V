{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Duplicate Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over 100 million people visit Quora every month, so it's no surprise that many people ask similar (or the same) questions. Various questions with the same intent can cause people to spend extra time searching for the best answer to their question, and make writers to answer multiple versions of the same question. Quora uses random forest to identify duplicated questions to provide a better experience to active seekers and writers, and offer more value to both of these groups in the long term.\n",
    "\n",
    "We are going to build the similar model during this project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\", encoding='UTF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[['question1', 'question2', 'is_duplicate']]\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question1  \\\n",
       "0  What is the step by step guide to invest in sh...   \n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2  How can I increase the speed of my internet co...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[['question1','question2']], data.is_duplicate, \n",
    "                                                    test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning\n",
    "\n",
    "- Tokenization\n",
    "- Stopwords cleaning\n",
    "- Removing punctuation\n",
    "- Normalizing\n",
    "- Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def punc_remove(docs):\n",
    "    review_no_puncs = []\n",
    "    for review in docs:\n",
    "        review_no_punc = ''.join([char for char in review if char not in string.punctuation])\n",
    "        review_no_puncs.append(review_no_punc)\n",
    "    return review_no_puncs\n",
    "\n",
    "def stop_word_remove(docs):\n",
    "    review_no_stops = []\n",
    "    for review in docs:\n",
    "        tokens = review.split()\n",
    "        review_no_stop = ' '.join([word for word in tokens if word not in stop_words])\n",
    "        review_no_stops.append(review_no_stop)\n",
    "    return review_no_stops\n",
    "\n",
    "def lemmitization(docs):\n",
    "    review_lemms = []\n",
    "    for review in docs:\n",
    "        tokens = review.split()\n",
    "        review_lemm = ' '.join([lemmatizer.lemmatize(word) for word in tokens])\n",
    "        review_lemms.append(review_lemm)\n",
    "    return review_lemms\n",
    "\n",
    "def stop_word_remove(docs):\n",
    "    review_no_stops = []\n",
    "    for review in docs:\n",
    "        tokens = review.split()\n",
    "        review_no_stop = ' '.join([word for word in tokens if word not in stop_words])\n",
    "        review_no_stops.append(review_no_stop)\n",
    "    return review_no_stops\n",
    "\n",
    "def tokenize(texts):\n",
    "    tokenized = []\n",
    "    for doc in texts:\n",
    "        tokenized.append(simple_preprocess(doc, min_len=2))\n",
    "    return tokenized\n",
    "\n",
    "def to_word_string(tokens):\n",
    "    texts = []\n",
    "    for doc in tokens:\n",
    "        texts.append(' '.join([word for word in doc]))\n",
    "    return texts\n",
    "\n",
    "def preprocess(texts):        \n",
    "    texts = punc_remove(texts)\n",
    "    texts = stop_word_remove(texts)\n",
    "    texts = stop_word_remove(texts)\n",
    "    texts = lemmitization(texts)\n",
    "    texts = tokenize(texts)\n",
    "    return texts\n",
    "\n",
    "def make_word_list(*args):\n",
    "    i = len(args)\n",
    "    word_list = set()\n",
    "    for i in range(i):\n",
    "        for e in args[i]:\n",
    "            word_list.update(e)\n",
    "    return list(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_tokens = preprocess(pd.concat([X_train['question1'], X_train['question2']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = to_word_string(qs_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_word_list = make_word_list(qs_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_train_tokens = preprocess(X_train['question1'])\n",
    "q2_train_tokens = preprocess(X_train['question2'])\n",
    "q1_test_tokens = preprocess(X_test['question1'])\n",
    "q2_test_tokens = preprocess(X_test['question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_train_str = to_word_string(q1_train_tokens)\n",
    "q2_train_str = to_word_string(q2_train_tokens)\n",
    "q1_test_str = to_word_string(q1_test_tokens)\n",
    "q2_test_str = to_word_string(q2_test_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "- tf-idf\n",
    "- word2vec\n",
    "- word count\n",
    "- number of the same words in both questions\n",
    "- ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect = TfidfVectorizer()\n",
    "def cosine_sim(text1, text2):\n",
    "    tfidf = vect.fit_transform([text1, text2])\n",
    "    return ((tfidf * tfidf.T).A)[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts1 = X_train.question1.values.tolist()\n",
    "train_texts2 = X_train.question2.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_targets = []\n",
    "for text1, text2 in zip(train_texts1, train_texts2):\n",
    "    X_train_targets.append(cosine_sim(text1, text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train, pd.Series(X_train_targets, name='cos_sim')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts1 = X_test.question1.values.tolist()\n",
    "test_texts2 = X_test.question2.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_targets = []\n",
    "for text1, text2 in zip(test_texts1, test_texts2):\n",
    "    X_test_targets.append(cosine_sim(text1, text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.concat([X_test, pd.Series(X_test_targets, name='cos_sim')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(lowercase=False, max_features=1500, min_df=0)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=0, lowercase=False, max_features=1500)\n",
    "vectorizer.fit(qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_q1 = vectorizer.transform(X_train.question1)\n",
    "x_train_q2 = vectorizer.transform(X_train.question2)\n",
    "x_test_q1 = vectorizer.transform(X_test.question1)\n",
    "x_test_q2 = vectorizer.transform(X_test.question2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "from scipy import sparse\n",
    "X_train_tfidf = hstack((x_train_q1,x_train_q2, \n",
    "                        sparse.csr_matrix(np.array(X_train_targets).reshape(-1, 1))))\n",
    "X_test_tfidf = hstack((x_test_q1,x_test_q2, \n",
    "                       sparse.csr_matrix(np.array(X_test_targets).reshape(-1, 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences=qs, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x2cb2e11c9d0>"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "Different modeling techniques can be used:\n",
    "\n",
    "- logistic regression\n",
    "- XGBoost\n",
    "- LSTMs\n",
    "- etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from scipy import sparse\n",
    "\n",
    "def main_process(X):\n",
    "    X.dropna(inplace=True)\n",
    "    \n",
    "    tokens = preprocess(pd.concat([X['question1'], X['question2']]))\n",
    "    qs = to_word_string(tokens)\n",
    "    \n",
    "    vectorizer = CountVectorizer(min_df=0, lowercase=False, max_features=3000)\n",
    "    vectorizer.fit(qs)\n",
    "    \n",
    "    x_q1 = vectorizer.transform(X.question1)\n",
    "    x_q2 = vectorizer.transform(X.question2)\n",
    "    \n",
    "    \n",
    "    train_texts1 = X.question1.values.tolist()\n",
    "    train_texts2 = X.question2.values.tolist()\n",
    "    \n",
    "    X_targets = []\n",
    "    for text1, text2 in zip(train_texts1, train_texts2):\n",
    "        X_targets.append(cosine_sim(text1, text2))\n",
    "    \n",
    "    X_tfidf = hstack((x_q1, x_q2, \n",
    "                      sparse.csc_matrix(np.array(X_targets).reshape(-1, 1))))\n",
    "    return X_tfidf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "prep = FunctionTransformer(main_process)\n",
    "model = LogisticRegression(max_iter=1700)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('pre', prep),\n",
    "    ('model', model)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('pre',\n",
       "                 FunctionTransformer(func=<function main_process at 0x000002CC72C89D30>)),\n",
       "                ('model', LogisticRegression(max_iter=1700))])"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.5952534072076975\n",
      "Recall:  0.6328439259855189\n",
      "Precision:  0.464393771677367\n",
      "F1 Score:  0.5356884443498616\n",
      "Confusion Matrix : \n",
      "[[29252 21774]\n",
      " [10953 18879]]\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Recall: ', recall_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred))\n",
    "print('F1 Score: ', f1_score(y_test, y_pred))\n",
    "print('Confusion Matrix : ') \n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1700)"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=1700)\n",
    "model.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.4529174602389374\n",
      "Recall:  0.999597747385358\n",
      "Precision:  0.40273350980498085\n",
      "F1 Score:  0.5741460972698217\n",
      "Confusion Matrix : \n",
      "[[ 6802 44224]\n",
      " [   12 29820]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n",
    "\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Recall: ', recall_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred))\n",
    "print('F1 Score: ', f1_score(y_test, y_pred))\n",
    "print('Confusion Matrix : ') \n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.47407801330727944\n",
      "Recall:  0.9924242424242424\n",
      "Precision:  0.4117377094777832\n",
      "F1 Score:  0.5820104779971889\n",
      "Confusion Matrix : \n",
      "[[ 8727 42299]\n",
      " [  226 29606]]\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(objective='reg:squarederror', n_estimators=500, use_label_encoder=False, \n",
    "                    max_depth=3)\n",
    "xgb.fit(X_train_tfidf, y_train)\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n",
    "\n",
    "y_pred = xgb.predict(X_test_tfidf)\n",
    "\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Recall: ', recall_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred))\n",
    "print('F1 Score: ', f1_score(y_test, y_pred))\n",
    "print('Confusion Matrix : ') \n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75674798, 0.75493924, 0.75928331, 0.75360975, 0.75594033])"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(xgb, X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(sentences_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "# X_test = tokenizer.texts_to_sequences(sentences_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 100\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "# X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 50\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=3,\n",
    "                    verbose=False,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10)\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "plot_history(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
